# K-Nearest Neighbors (KNN)

![image](images/knn.png)

## KNN Model Representation

The model representation for **KNN** is the **entire training dataset**. It is as simple as that.
**KNN** has **no model** other than **storing** the **entire dataset**, so there is **no learning** required.

Efficient implementations can store the data using complex data structures like **k-d trees** to make look-up and matching of new patterns during prediction efficiently.

Because the entire training dataset is stored, you may want to think carefully about the consistency of your training data. It might be a good idea to curate it, update it often as new data becomes available and remove erroneous and outlier data.

## Making Predictions with KNN

**KNN** makes predictions using the **training dataset directly**.
Predictions are made for a new instance (**x**) by **searching** through the entire training set for the **K** **most similar** instances (the **neighbors**) and summarizing the output variable for those **K** instances.

* For **regression** this might be the **mean** output variable.
* In **classification** this might be the **mode** (or **most common**) class value.

To determine which of the **K** instances in the training dataset are most **similar** to a new input a **distance measure** is used. For real-valued input variables, the most popular distance measure is **Euclidean distance**.

**Euclidean distance** is calculated as the square root of the sum of the squared differences between a new point (**x**) and an existing point (**xi**) across all input attributes **j**.

> **EuclideanDistance(x, xi) = sqrt( sum( (xj – xij)^2 ) )**

Other popular distance measures include:

* **Hamming Distance**: Calculate the distance between binary vectors.
* **Manhattan Distance**: Calculate the distance between real vectors using the sum of their absolute difference. Also called City Block Distance.
* **Minkowski Distance**: Generalization of Euclidean and Manhattan distance.

There are many other distance measures that can be used, such as:

* **Tanimoto**.
* **Jaccard**.
* **Mahalanobis**.
* **Cosine distance**.

You can choose the best distance metric based on the properties of your data. If you are unsure, you can experiment with different distance metrics and different values of **K** together and see which mix results in the most accurate models.

* **Euclidean** is a good distance measure to use if the input variables are **similar in type** (e.g. all measured widths and heights).

* **Manhattan distance** is a good measure to use if the input variables are **not similar** in type (such as age, gender, height, etc.).

The value for **K** can be found by **algorithm tuning**.
It is a good idea to try many different values for **K** (e.g. values from **1** to **21**) and see what works best for your problem.

The **computational complexity** of **KNN** increases with the **size** of the training dataset.
For very large training sets, **KNN** can be made **stochastic** by taking a sample from the training dataset from which to calculate the K-most similar instances.

KNN has been around for a long time and has been very well studied. As such, different disciplines have different names for it, for example:

* **Instance-Based Learning**: The raw training instances are used to make predictions. As such KNN is often referred to as **instance-based learning** or a **case-based learning** (where each training instance is a case from the problem domain).
* **Lazy Learning**: No learning of the model is required and all of the work happens at the time a prediction is requested. As such, KNN is often referred to as a lazy learning algorithm.
* **Non-Parametric**: KNN makes no assumptions about the functional form of the problem being solved. As such KNN is referred to as a non-parametric machine learning algorithm.

KNN can be used for **regression** and **classification** problems.

## KNN for Regression

When KNN is used for regression problems the prediction is based on the **mean** or the **median** of the K-most similar instances.

## KNN for Classification

When KNN is used for classification, the output can be calculated as the class with the **highest frequency** from the K-most similar instances. Each instance in essence votes for their class and the class with the most votes is taken as the prediction.

**Class probabilities** can be calculated as the **normalized frequency** of samples that belong to each class in the **set** of **K most similar** instances for a new data instance.
For example, in a **binary classification** problem (class is **0** or **1**):

> **p(class=0) = count(class=0) / (count(class=0) + count(class=1))**

If you are using **K** and you have an **even number** of classes (e.g. **2**) _it is a good idea to choose a **K** value with an **odd number** to avoid a **tie**_.
And the inverse, use an **even number** for **K** when you have an **odd number of classes**.

**Ties** can be broken consistently by expanding **K** by **1** and looking at the class of the next most similar instance in the training dataset.

## Curse of Dimensionality

KNN works well with a small number of input variables (**p**), but struggles when the number of inputs is very large.

Each input variable can be considered a **dimension** of a **p-dimensional** input space. For example, if you had two input variables **x1** and **x2**, the input space would be 2-dimensional.
As the number of dimensions increases the volume of the input space increases at an exponential rate.

In **high dimensions**, points that _may be similar may have very large distances_. All points will be far away from each other and our intuition for distances in simple **2** and **3-dimensional spaces** breaks down.

This might feel unintuitive at first, but this general problem is called the **“Curse of Dimensionality“**.

## Best Prepare Data for KNN

* **Rescale Data**: KNN performs much better if all of the data has the same scale. Normalizing your data to the range **[0, 1]** is a good idea. It may also be a good idea to **standardize** your data if it has a **Gaussian distribution**.
* **Address Missing Data**: Missing data will mean that the distance between samples can not be calculated. These samples could be excluded or the missing values could be imputed.
* **Lower Dimensionality**: KNN is suited for lower dimensional data. You can try it on high dimensional data (hundreds or thousands of input variables) but be aware that it may not perform as well as other techniques. KNN can benefit from **feature selection** that reduces the dimensionality of the input feature space.
